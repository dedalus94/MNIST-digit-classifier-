{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In this project, you will build a neural network of your own design to evaluate the MNIST dataset.\n",
    "\n",
    "Some of the benchmark results on MNIST include can be found [on Yann LeCun's page](http://yann.lecun.com/exdb/mnist/) and include:\n",
    "\n",
    "88% [Lecun et al., 1998](http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf)\n",
    "95.3% [Lecun et al., 1998](http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf)\n",
    "99.65% [Ciresan et al., 2011](http://people.idsia.ch/~juergen/ijcai2011.pdf)\n",
    "\n",
    "MNIST is a great dataset for sanity checking your models, since the accuracy levels achieved by large convolutional neural networks and small linear models are both quite high. This makes it important to be familiar with the data.\n",
    "\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This cell contains the essential imports you will need – DO NOT CHANGE THE CONTENTS! ##\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Dataset\n",
    "\n",
    "Specify your transforms as a list if you intend to .\n",
    "The transforms module is already loaded as `transforms`.\n",
    "\n",
    "MNIST is fortunately included in the torchvision module.\n",
    "Then, you can create your dataset using the `MNIST` object from `torchvision.datasets` ([the documentation is available here](https://pytorch.org/vision/stable/datasets.html#mnist)).\n",
    "Make sure to specify `download=True`! \n",
    "\n",
    "Once your dataset is created, you'll also need to define a `DataLoader` from the `torch.utils.data` module for both the train and the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torchvision/datasets/mnist.py:498: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ../torch/csrc/utils/tensor_numpy.cpp:178.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    }
   ],
   "source": [
    "data_pre_process = datasets.MNIST(root=\"data\", train=True, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  38,  43, 105,\n",
       "         255, 253, 253, 253, 253, 253, 174,   6,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,  43, 139, 224, 226, 252,\n",
       "         253, 252, 252, 252, 252, 252, 252, 158,  14,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0, 178, 252, 252, 252, 252,\n",
       "         253, 252, 252, 252, 252, 252, 252, 252,  59,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0, 109, 252, 252, 230, 132,\n",
       "         133, 132, 132, 189, 252, 252, 252, 252,  59,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   4,  29,  29,  24,   0,\n",
       "           0,   0,   0,  14, 226, 252, 252, 172,   7,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,  85, 243, 252, 252, 144,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,  88, 189, 252, 252, 252,  14,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          91, 212, 247, 252, 252, 252, 204,   9,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,  32, 125, 193, 193, 193,\n",
       "         253, 252, 252, 252, 238, 102,  28,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,  45, 222, 252, 252, 252, 252,\n",
       "         253, 252, 252, 252, 177,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,  45, 223, 253, 253, 253, 253,\n",
       "         255, 253, 253, 253, 253,  74,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,  31, 123,  52,  44,  44,\n",
       "          44,  44, 143, 252, 252,  74,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,  15, 252, 252,  74,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,  86, 252, 252,  74,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   5,  75,   9,   0,   0,   0,   0,   0,\n",
       "           0,  98, 242, 252, 252,  74,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,  61, 183, 252,  29,   0,   0,   0,   0,  18,\n",
       "          92, 239, 252, 252, 243,  65,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0, 208, 252, 252, 147, 134, 134, 134, 134, 203,\n",
       "         253, 252, 252, 188,  83,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0, 208, 252, 252, 252, 252, 252, 252, 252, 252,\n",
       "         253, 230, 153,   8,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,  49, 157, 252, 252, 252, 252, 252, 217, 207,\n",
       "         146,  45,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   7, 103, 235, 252, 172, 103,  24,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0]],\n",
       "       dtype=torch.uint8)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_pre_process.data[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(33.3184) tensor(78.5675)\n",
      "tensor(33.7912) tensor(79.1725)\n"
     ]
    }
   ],
   "source": [
    "# Define transforms\n",
    "\n",
    "def transform_data(train):\n",
    "    \n",
    "    \n",
    "    # calculate mean and std first, without downloading the data\n",
    "    # these can be different values for Test and Training sets \n",
    "    \n",
    "    data_set = datasets.MNIST(root=\"data\", train=train, download=True, transform=transforms.ToTensor())\n",
    "    mean = (torch.mean(data_set.data.float()))\n",
    "    std = (torch.std(data_set.data.float()))\n",
    "    print(mean,std)\n",
    "    #use mean and std to normalise plus auto augment to reduce overfitting\n",
    "    \n",
    "    transformations = transforms.Compose([transforms.AutoAugment(),\n",
    "                                          transforms.ToTensor(),\n",
    "                                          transforms.Normalize(mean,std),\n",
    "                               ])\n",
    "\n",
    "    data_set_norm = datasets.MNIST(root=\"data\", train=train, download=True, transform=transformations)\n",
    "    \n",
    "    return data_set_norm\n",
    "\n",
    "\n",
    "\n",
    "# Create training set and define training dataloader\n",
    "training_data = transform_data(True)\n",
    "test_data  = transform_data(False)\n",
    "\n",
    "\n",
    "# Create test set and define test dataloader\n",
    "train_loader = DataLoader(training_data, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  38,  43, 105,\n",
       "         255, 253, 253, 253, 253, 253, 174,   6,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,  43, 139, 224, 226, 252,\n",
       "         253, 252, 252, 252, 252, 252, 252, 158,  14,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0, 178, 252, 252, 252, 252,\n",
       "         253, 252, 252, 252, 252, 252, 252, 252,  59,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0, 109, 252, 252, 230, 132,\n",
       "         133, 132, 132, 189, 252, 252, 252, 252,  59,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   4,  29,  29,  24,   0,\n",
       "           0,   0,   0,  14, 226, 252, 252, 172,   7,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,  85, 243, 252, 252, 144,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,  88, 189, 252, 252, 252,  14,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          91, 212, 247, 252, 252, 252, 204,   9,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,  32, 125, 193, 193, 193,\n",
       "         253, 252, 252, 252, 238, 102,  28,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,  45, 222, 252, 252, 252, 252,\n",
       "         253, 252, 252, 252, 177,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,  45, 223, 253, 253, 253, 253,\n",
       "         255, 253, 253, 253, 253,  74,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,  31, 123,  52,  44,  44,\n",
       "          44,  44, 143, 252, 252,  74,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,  15, 252, 252,  74,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,  86, 252, 252,  74,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   5,  75,   9,   0,   0,   0,   0,   0,\n",
       "           0,  98, 242, 252, 252,  74,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,  61, 183, 252,  29,   0,   0,   0,   0,  18,\n",
       "          92, 239, 252, 252, 243,  65,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0, 208, 252, 252, 147, 134, 134, 134, 134, 203,\n",
       "         253, 252, 252, 188,  83,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0, 208, 252, 252, 252, 252, 252, 252, 252, 252,\n",
       "         253, 230, 153,   8,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,  49, 157, 252, 252, 252, 252, 252, 217, 207,\n",
       "         146,  45,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   7, 103, 235, 252, 172, 103,  24,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0]],\n",
       "       dtype=torch.uint8)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data.data[7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Justify your preprocessing\n",
    "\n",
    "I just used auto augment to reduce the likelyhood of overfitting "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the Dataset\n",
    "Using matplotlib, numpy, and torch, explore the dimensions of your data.\n",
    "\n",
    "You can view images using the `show5` function defined below – it takes a data loader as an argument.\n",
    "Remember that normalized images will look really weird to you! You may want to try changing your transforms to view images.\n",
    "Typically using no transforms other than `toTensor()` works well for viewing – but not as well for training your network.\n",
    "If `show5` doesn't work, go back and check your code for creating your data loaders and your training/test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This cell contains a function for showing 5 images from a dataloader – DO NOT CHANGE THE CONTENTS! ##\n",
    "def show5(img_loader):\n",
    "    dataiter = iter(img_loader)\n",
    "    \n",
    "    batch = next(dataiter)\n",
    "    labels = batch[1][0:5]\n",
    "    images = batch[0][0:5]\n",
    "    for i in range(5):\n",
    "        print(int(labels[i].detach()))\n",
    "    \n",
    "        image = images[i].numpy()\n",
    "        #print(image.mode)\n",
    "        plt.imshow(image.T.squeeze().T)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Only torch.uint8 image tensors are supported, but found torch.float32",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-85-6e666ff8272c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Explore data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mshow5\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-83-d0aba28e8bb2>\u001b[0m in \u001b[0;36mshow5\u001b[0;34m(img_loader)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mdataiter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataiter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    528\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    568\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torchvision/datasets/mnist.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_transform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torchvision/transforms/autoaugment.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m    220\u001b[0m                     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madjust_sharpness\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmagnitude\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mop_name\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"Posterize\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m                     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposterize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagnitude\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mop_name\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"Solarize\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m                     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msolarize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmagnitude\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mposterize\u001b[0;34m(img, bits)\u001b[0m\n\u001b[1;32m   1282\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mF_pil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposterize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1284\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mF_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposterize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torchvision/transforms/functional_tensor.py\u001b[0m in \u001b[0;36mposterize\u001b[0;34m(img, bits)\u001b[0m\n\u001b[1;32m    849\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Input image tensor should have at least 3 dimensions, but found {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 851\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Only torch.uint8 image tensors are supported, but found {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    852\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m     \u001b[0m_assert_channels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Only torch.uint8 image tensors are supported, but found torch.float32"
     ]
    }
   ],
   "source": [
    "# Explore data\n",
    "show5(test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build your Neural Network\n",
    "Using the layers in `torch.nn` (which has been imported as `nn`) and the `torch.nn.functional` module (imported as `F`), construct a neural network based on the parameters of the dataset.\n",
    "Use any architecture you like. \n",
    "\n",
    "*Note*: If you did not flatten your tensors in your transforms or as part of your preprocessing and you are using only `Linear` layers, make sure to use the `Flatten` layer in your network!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "784"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "28*28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the class for your neural network\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.activation = F.relu\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.layer1 = nn.Linear(28*28,32)\n",
    "        self.layer2 = nn.Linear(32,10) \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = self.activation(self.layer1(x))\n",
    "        #x = self.dropout(x)\n",
    "        x = self.layer2(x) \n",
    "       \n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running your Neural Network\n",
    "Use whatever method you like to train your neural network, and ensure you record the average loss at each epoch. \n",
    "Don't forget to use `torch.device()` and the `.to()` method for both your model and your data if you are using GPU!\n",
    "\n",
    "If you want to print your loss **during** each epoch, you can use the `enumerate` function and print the loss after a set number of batches. 250 batches works well for most people!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 training accuracy: 25.08% training loss: 0.91400\n",
      "Epoch 1 validation accuracy: 28.42% validation loss: 0.42068\n",
      "Epoch 2 training accuracy: 28.58% training loss: 0.38791\n",
      "Epoch 2 validation accuracy: 28.95% validation loss: 0.33516\n",
      "Epoch 3 training accuracy: 28.95% training loss: 0.33568\n",
      "Epoch 3 validation accuracy: 29.20% validation loss: 0.30804\n",
      "Epoch 4 training accuracy: 29.20% training loss: 0.30904\n",
      "Epoch 4 validation accuracy: 29.45% validation loss: 0.28608\n",
      "Epoch 5 training accuracy: 29.35% training loss: 0.29022\n",
      "Epoch 5 validation accuracy: 29.50% validation loss: 0.27289\n",
      "Epoch 6 training accuracy: 29.51% training loss: 0.27468\n",
      "Epoch 6 validation accuracy: 29.58% validation loss: 0.26148\n",
      "Epoch 7 training accuracy: 29.61% training loss: 0.26123\n",
      "Epoch 7 validation accuracy: 29.72% validation loss: 0.24844\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "int() argument must be a string, a bytes-like object or a number, not 'Image'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-151146c268d2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     97\u001b[0m                                                      \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m                                                      \u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m                                                      train_loader)    \n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-151146c268d2>\u001b[0m in \u001b[0;36mtraining\u001b[0;34m(num_epochs, optimizer, criterion, net, train_loader)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mtrain_correct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0;31m# data is a list of [inputs, labels]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    528\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    568\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torchvision/datasets/mnist.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_transform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m     95\u001b[0m             \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mConverted\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \"\"\"\n\u001b[0;32m---> 97\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mto_tensor\u001b[0;34m(pic)\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0mmode_to_nptype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'I'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'I;16'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'F'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m     img = torch.from_numpy(\n\u001b[0;32m--> 129\u001b[0;31m         \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode_to_nptype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m     )\n\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: int() argument must be a string, a bytes-like object or a number, not 'Image'"
     ]
    }
   ],
   "source": [
    "def training(num_epochs,optimizer,criterion,net,train_loader):\n",
    "    \n",
    "    #check for GPU \n",
    "    if torch.cuda.is_available():\n",
    "        # move tensor to CUDA\n",
    "        device = torch.device(\"cuda\")\n",
    "        \n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "\n",
    "    net=net.to(device) \n",
    "    \n",
    "    #tensorboard!!!\n",
    "    #writer = SummaryWriter()\n",
    "    \n",
    "    #keep track of train and val loss:\n",
    "    train_loss_history = list()\n",
    "    val_loss_history = list()\n",
    "\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        net.train() # TRAIN MODE\n",
    "\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "\n",
    "        for i, data in enumerate(train_loader):\n",
    "\n",
    "            # data is a list of [inputs, labels]\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device) \n",
    "\n",
    "            # Zero out the gradients of the optimizer\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Get the outputs of your model and compute your loss\n",
    "            outputs = net(inputs)\n",
    "            #print(outputs.shape)\n",
    "            #print(labels.shape)\n",
    "            loss = criterion(outputs,labels)\n",
    "\n",
    "            # Compute the loss gradient using the backward method and have the optimizer take a step\n",
    "            loss.backward()\n",
    "            \n",
    "            #### TENSOR BOARD\"\n",
    "            \"\"\"\n",
    "            for name, param in net.named_parameters():\n",
    "                \n",
    "                writer.add_histogram(name + '_weights', param, epoch)\n",
    "                writer.add_histogram(name + '_gradients', param.grad, epoch)\n",
    "            \"\"\"\n",
    "            \n",
    "            nn.utils.clip_grad_norm_(net.parameters(), 2)\n",
    "            \n",
    "            # actually updates the weights, taking into account the learn rate and momentum:\n",
    "            optimizer.step()\n",
    "\n",
    "            # Compute the accuracy and print the accuracy and loss\n",
    "            _, preds = torch.max(outputs.data, 1)\n",
    "            train_correct += (preds == labels).sum().item()\n",
    "            train_loss += loss.item()\n",
    "        print(f'Epoch {epoch + 1} training accuracy: {train_correct/len(train_loader):.2f}% training loss: {train_loss/len(train_loader):.5f}')\n",
    "        train_loss_history.append(train_loss/len(train_loader))\n",
    "        \n",
    "        #writer.close()\n",
    "        \n",
    "        # Validation step \n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        net.eval()\n",
    "        for inputs, labels in test_loader:\n",
    "\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            outputs = net(inputs).to(device)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            _, preds = torch.max(outputs.data, 1)\n",
    "            val_correct += (preds == labels).sum().item()\n",
    "            val_loss += loss.item()\n",
    "        print(f'Epoch {epoch + 1} validation accuracy: {val_correct/len(test_loader):.2f}% validation loss: {val_loss/len(test_loader):.5f}')\n",
    "        val_loss_history.append(val_loss/len(test_loader))\n",
    "    \n",
    "    \n",
    "    return net, train_loss_history, val_loss_history\n",
    "\n",
    "net = Net()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01, momentum=0)\n",
    "#optimizer = optim.Adam(net.parameters(), lr= 0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "    \n",
    "net, train_loss_history, val_loss_history = training(15,\n",
    "                                                     optimizer,\n",
    "                                                     criterion,\n",
    "                                                     net,\n",
    "                                                     train_loader)    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the training loss (and validation loss/accuracy, if recorded)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! tensorboard --logdir=/path/to/log/directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the training and validation loss history\n",
    "plt.plot(train_loss_history, label=\"Training Loss\")\n",
    "plt.plot(val_loss_history, label=\"Validation Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing your model\n",
    "Using the previously created `DataLoader` for the test set, compute the percentage of correct predictions using the highest probability prediction. \n",
    "\n",
    "If your accuracy is over 90%, great work, but see if you can push a bit further! \n",
    "If your accuracy is under 90%, you'll need to make improvements.\n",
    "Go back and check your model architecture, loss function, and optimizer to make sure they're appropriate for an image classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving your model\n",
    "\n",
    "Once your model is done training, try tweaking your hyperparameters and training again below to improve your accuracy on the test set!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving your model\n",
    "Using `torch.save`, save your model for future loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE ##"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
